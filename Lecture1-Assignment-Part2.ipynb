{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Movie Review Classification\n",
    "\n",
    "![This is getting exciting](https://i.kinja-img.com/gawker-media/image/upload/s--hIgTSFEs--/c_fit,fl_progressive,q_80,w_320/17j2zn73qxdlfgif.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using all that we have learned, we will now combine our techniques to perform some basic classifcation! We'll be using the nltk movie reviews data set, we will classify positive and negative reviews. Here's some code to get you started:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import movie_reviews as reviews\n",
    "\n",
    "X = [reviews.raw(fileid) for fileid in reviews.fileids()]\n",
    "y = [reviews.categories(fileid)[0] for fileid in reviews.fileids()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1 - Print a positive and negative review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "**** POSTIVE REVIEW ****\n",
      "films adapted from comic books have had plenty of success , whether they're about superheroes ( batman , superman , spawn ) , or geared toward kids ( casper ) or the arthouse crowd ( ghost world ) , but there's never really been a comic book like from hell before . \n",
      "for starters , it was created by \n",
      "\n",
      "**** NEGATIVE REVIEW ****\n",
      "plot : two teen couples go to a church party , drink and then drive . \n",
      "they get into an accident . \n",
      "one of the guys dies , but his girlfriend continues to see him in her life , and has nightmares . \n",
      "what's the deal ? \n",
      "watch the movie and \" sorta \" find out . . . \n",
      "critique : a mind-fuck movie for the\n"
     ]
    }
   ],
   "source": [
    "# Print a positive movie review\n",
    "print('\\n**** POSTIVE REVIEW ****')\n",
    "print(X[y.index('pos')][:300])\n",
    "\n",
    "# Print a negative movie review\n",
    "print('\\n**** NEGATIVE REVIEW ****')\n",
    "print(X[y.index('neg')][:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Using the scikit train_test_split function (http://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html), split the data into a training set and a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Reviews:  1600   Testing Reviews:  400\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split the revews into train and test partitions\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=11)\n",
    "print('Training Reviews: ', len(X_train), '  Testing Reviews: ', len(X_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Then lemmatize or stem the reviews, and transform the documents to tf-idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** There are 1316846 items in the reviews vocabulary.\n",
      "           words\n",
      "are          are\n",
      "of            of\n",
      "an            an\n",
      "belong   belongs\n",
      "bear        bear\n",
      "is            is\n",
      "be         being\n",
      "script  scripted\n",
      "if            if\n",
      "in            in\n",
      "   \n",
      "*** There are 41463 tokenized features. \n",
      " \n",
      "Smallest TfIdf: \n",
      "['mikel' 'robinson-blackmore' 'koven' 'micheal' 'esmeralda' 'cradles'\n",
      " 'pig-keeper' 'proudest' '25th' 'bardsley']\n",
      "\n",
      "Largest TfIdf: \n",
      "['nbsp' 'webb' 'leila' 'pokemon' 'alessa' 'bye' 'flynt' 'matilda' 'rocky'\n",
      " 'giles']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn import feature_extraction\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Define stopwords and stemmer\n",
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = SnowballStemmer('english')\n",
    "\n",
    "# Define tokenize and stem functions \n",
    "def tokenize_and_stem(text):\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "def tokenize_only(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens\n",
    "\n",
    "# tokenize and stem the reviews\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "for i in X:\n",
    "    allwords_stemmed = tokenize_and_stem(i) \n",
    "    totalvocab_stemmed.extend(allwords_stemmed) \n",
    "    \n",
    "    allwords_tokenized = tokenize_only(i)\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "# Print a sample of the stems and words\n",
    "vocab_df = pd.DataFrame({'words': totalvocab_tokenized}, index = totalvocab_stemmed)\n",
    "print('*** There are ' + str(vocab_df.shape[0]) + ' items in the reviews vocabulary.')\n",
    "print(vocab_df.sample(10))\n",
    "\n",
    "# Configure TiIdf Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords,\n",
    "                             tokenizer=tokenize_only)\n",
    " \n",
    "# Learn and transform train documents\n",
    "X_train_tfidf  = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print ('   ')\n",
    "print ('*** There are ' + str(len(feature_names)) + ' tokenized features. ')\n",
    "\n",
    "# Extract smallest and largest features by tfidf\n",
    "print (\" \")\n",
    "sorted_tfidf_index = X_train_tfidf.max(0).toarray()[0].argsort()\n",
    "print('Smallest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Finally, build a model. To start, use a logistic regression (which we will review in detail in the coming lectures) \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Transform  sentiment classes/labels to binary values\n",
    "y_train_bin = [1 if x == 'pos' else 0 for x in y_train]\n",
    "y_test_bin = [1 if x == 'pos' else 0 for x in y_test]\n",
    "\n",
    "# Define and train logit_model using tfidf vectorized training data\n",
    "logit_model = LogisticRegression()\n",
    "logit_model.fit(X_train_tfidf, y_train_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 - Measure the efficacy of your model using the Reciever Operator Characteristic (ROC) Area Under the Curve (AUC). Report this metric on the test set of your data.\n",
    "\n",
    "For more info on this, see: http://scikit-learn.org/stable/auto_examples/model_selection/plot_roc_crossval.html#sphx-glr-auto-examples-model-selection-plot-roc-crossval-py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1. Standard logit model using tokenized only vectorization of features.\n",
      "ROC AUC score 1:  0.822951844903\n",
      "  \n",
      "Correctly predicts sample as:\n",
      "The movie is really bad, I will never recommend it  ||| The movie was sad, but really great ending.  I will see it again\n",
      "[0 1]\n"
     ]
    }
   ],
   "source": [
    "# Predict test review sentiment class based on logit model\n",
    "logit_predict = logit_model.predict(X_test_tfidf)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "roc_auc_1 = roc_auc_score(y_test_bin, logit_predict)\n",
    "print('Model 1. Standard logit model using tokenized only vectorization of features.')\n",
    "print('ROC AUC score 1: ', roc_auc_1)\n",
    "print('  ')\n",
    "print ('Correctly predicts sample as:')\n",
    "print('The movie is really bad, I will never recommend it',' ||| The movie was sad, but really great ending.  I will see it again')\n",
    "print(logit_model.predict(vectorizer.transform(['The movie is really bad, I will never recommend it.','The movie was sad, but really great ending.  I will see it again.'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6 - Change a parameter in your model (introduce regularization) or change a parameter in your word vector transformation\n",
    "(http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html). Try introducing the use of stop words, or employing a cutoff on terms with min or max df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** There are 28649 tokenized features. \n",
      " \n",
      "Smallest TfIdf: \n",
      "['koven' 'mikel' 'robinson-blackmor' '`never' 'dragonlik' 'unshroud'\n",
      " 'fondest' 'heartbroken' 'fondacairo' '`disneyfi']\n",
      "\n",
      "Largest TfIdf: \n",
      "['pimp' 'nbsp' 'webb' 'pokemon' 'alessa' 'leila' 'bye' 'matilda' 'flynt'\n",
      " 'gile']\n",
      "\n",
      "Model 2. Standard logit model using tokenized and stemming vectorization of features.\n",
      "ROC AUC score 2:  0.827829893684\n"
     ]
    }
   ],
   "source": [
    "# Configure TiIdf Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords,\n",
    "                             tokenizer=tokenize_and_stem)\n",
    " \n",
    "# Learn and transform train documents\n",
    "X_train_tfidf  = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print ('*** There are ' + str(len(feature_names)) + ' tokenized features. ')\n",
    "\n",
    "# Extract smallest and largest features by tfidf\n",
    "print (\" \")\n",
    "sorted_tfidf_index = X_train_tfidf.max(0).toarray()[0].argsort()\n",
    "print('Smallest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n",
    "\n",
    "# Build the logit model\n",
    "logit_model.fit(X_train_tfidf, y_train_bin)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "logit_predict = logit_model.predict(X_test_tfidf)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "roc_auc_2 = roc_auc_score(y_test_bin, logit_predict)\n",
    "print('Model 2. Standard logit model using tokenized and stemming vectorization of features.')\n",
    "print('ROC AUC score 2: ', roc_auc_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7 - Make four models in total, changing parameters and comparing the AUC results. Report your findings in a tabular form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** There are 422 tokenized features. \n",
      " \n",
      "Smallest TfIdf: \n",
      "['already' 'later' 'based' 'left' 'interest' 'directed' 'sort' 'wonder'\n",
      " 'anyone' 'nearly']\n",
      "\n",
      "Largest TfIdf: \n",
      "['family' 'star' 'classic' 'go' 'city' \"'m\" 'evil' 'black' 'run' 'series']\n",
      "\n",
      "Model 3. Standard logit model using tokenize only and min/max freq of .1/.7 vectorization of features.\n",
      "ROC AUC score 3:  0.780988117573\n"
     ]
    }
   ],
   "source": [
    "# Configure TiIdf Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords,\n",
    "                             tokenizer=tokenize_only,\n",
    "                             min_df = .1, max_df = 0.7)\n",
    " \n",
    "# Learn and transform train documents\n",
    "X_train_tfidf  = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print ('*** There are ' + str(len(feature_names)) + ' tokenized features. ')\n",
    "\n",
    "# Extract smallest and largest features by tfidf\n",
    "print (\" \")\n",
    "sorted_tfidf_index = X_train_tfidf.max(0).toarray()[0].argsort()\n",
    "print('Smallest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n",
    "\n",
    "# Build the logit model\n",
    "logit_model.fit(X_train_tfidf, y_train_bin)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "logit_predict = logit_model.predict(X_test_tfidf)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "roc_auc_3 = roc_auc_score(y_test_bin, logit_predict)\n",
    "print('Model 3. Standard logit model using tokenize only and min/max freq of .1/.7 vectorization of features.')\n",
    "print('ROC AUC score 3: ', roc_auc_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** There are 1022530 tokenized features. \n",
      " \n",
      "Smallest TfIdf: \n",
      "['project minds nbsp' 'nbsp finally get' 'nbsp first'\n",
      " 'nbsp first screenplay' 'nbsp five' 'nbsp five years' 'nbsp given'\n",
      " 'nbsp given enough' 'nbsp however' 'nbsp however rather']\n",
      "\n",
      "Largest TfIdf: \n",
      "['nbsp' 'leila' 'webb' 'flynt' 'kermit' 'paulie' 'alessa' 'matilda' 'giles'\n",
      " 'ghost dog']\n",
      "\n",
      "Model 4. Standard logit model using tokenize only and ngram 1to3 vectorization.\n",
      "ROC AUC score 4:  0.825641025641\n"
     ]
    }
   ],
   "source": [
    "# Configure TiIdf Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords,\n",
    "                             tokenizer=tokenize_only,\n",
    "                             ngram_range = (1,3))\n",
    " \n",
    "# Learn and transform train documents\n",
    "X_train_tfidf  = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print ('*** There are ' + str(len(feature_names)) + ' tokenized features. ')\n",
    "\n",
    "# Extract smallest and largest features by tfidf\n",
    "print (\" \")\n",
    "sorted_tfidf_index = X_train_tfidf.max(0).toarray()[0].argsort()\n",
    "print('Smallest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n",
    "\n",
    "# Build the logit model\n",
    "logit_model.fit(X_train_tfidf, y_train_bin)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "logit_predict = logit_model.predict(X_test_tfidf)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "roc_auc_4 = roc_auc_score(y_test_bin, logit_predict)\n",
    "print('Model 4. Standard logit model using tokenize only and ngram 1to3 vectorization.')\n",
    "print('ROC AUC score 4: ', roc_auc_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** There are 5000 tokenized features. \n",
      " \n",
      "Smallest TfIdf: \n",
      "['characterist' 'peac' 'paragraph' 'februari' 'ensur' 'forewarn' 'incap'\n",
      " 'releg' 'earnest' 'notion']\n",
      "\n",
      "Largest TfIdf: \n",
      "['pimp' 'webb' 'nbsp' 'gile' 'bye' 'matilda' 'rocki' 'flynt' 'leila'\n",
      " 'pollock']\n",
      "\n",
      "Model 5. Standard logit model using tokenize / stem and ngram 1to3 vectorization.\n",
      "ROC AUC score 5:  0.8301438399\n"
     ]
    }
   ],
   "source": [
    "# Configure TiIdf Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stopwords,\n",
    "                             tokenizer=tokenize_and_stem,\n",
    "                             max_features = 5000)\n",
    "\n",
    " \n",
    "# Learn and transform train documents\n",
    "X_train_tfidf  = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "\n",
    "# Extract feature names\n",
    "feature_names = np.array(vectorizer.get_feature_names())\n",
    "print ('*** There are ' + str(len(feature_names)) + ' tokenized features. ')\n",
    "\n",
    "# Extract smallest and largest features by tfidf\n",
    "print (\" \")\n",
    "sorted_tfidf_index = X_train_tfidf.max(0).toarray()[0].argsort()\n",
    "print('Smallest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:10]]))\n",
    "print('Largest TfIdf: \\n{}\\n'.format(feature_names[sorted_tfidf_index[:-11:-1]]))\n",
    "\n",
    "# Build the logit model\n",
    "logit_model.fit(X_train_tfidf, y_train_bin)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "logit_predict = logit_model.predict(X_test_tfidf)\n",
    "\n",
    "# Predict test review sentiment class based on logit model\n",
    "roc_auc_5 = roc_auc_score(y_test_bin, logit_predict)\n",
    "print('Model 5. Standard logit model using tokenize / stem and ngram 1to3 vectorization.')\n",
    "print('ROC AUC score 5: ', roc_auc_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th># Features</th>\n",
       "      <th>ROC AUC Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>#5 - Tokenize and Stem w/ max features = 5000</td>\n",
       "      <td>5000</td>\n",
       "      <td>0.830144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>#2 - Tokenize and stemming</td>\n",
       "      <td>28649</td>\n",
       "      <td>0.827830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>#4 - Tokenize w/ Ngram=Trigram level</td>\n",
       "      <td>1022539</td>\n",
       "      <td>0.825641</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>#1 - TdIdf - Tokenization Only</td>\n",
       "      <td>41463</td>\n",
       "      <td>0.822952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>#3 - Tokenize w/ doc freq min/max = .1/.7</td>\n",
       "      <td>422</td>\n",
       "      <td>0.780988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Model  # Features  ROC AUC Score\n",
       "4  #5 - Tokenize and Stem w/ max features = 5000        5000       0.830144\n",
       "1                     #2 - Tokenize and stemming       28649       0.827830\n",
       "3           #4 - Tokenize w/ Ngram=Trigram level     1022539       0.825641\n",
       "0                 #1 - TdIdf - Tokenization Only       41463       0.822952\n",
       "2      #3 - Tokenize w/ doc freq min/max = .1/.7         422       0.780988"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import OrderedDict\n",
    "\n",
    "results = OrderedDict([ ('Model', ['#1 - TdIdf - Tokenization Only', '#2 - Tokenize and stemming', \n",
    "                                   '#3 - Tokenize w/ doc freq min/max = .1/.7', '#4 - Tokenize w/ Ngram=Trigram level',\n",
    "                                   '#5 - Tokenize and Stem w/ max features = 5000']),\n",
    "          ('# Features', [41463, 28649, 422, 1022539, 5000]),\n",
    "          ('ROC AUC Score', [roc_auc_1, roc_auc_2, roc_auc_3, roc_auc_4, roc_auc_5]) ])\n",
    "\n",
    "results_df = pd.DataFrame.from_dict(results)\n",
    "results_score_df = results_df.sort_values(by=['ROC AUC Score'],ascending=False)\n",
    "results_score_df[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### End of Assignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
